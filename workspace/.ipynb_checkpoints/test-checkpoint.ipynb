{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FH_7806leM5h","executionInfo":{"status":"ok","timestamp":1625119192636,"user_tz":-330,"elapsed":20633,"user":{"displayName":"Harshit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgybYzIih0sZffb4k2rPOU2ZTmImKuEXP5YNWiwYQ=s64","userId":"01054375526835731229"}},"outputId":"fb0bb37e-2e48-45fc-c1b6-9a1f93a8cd31"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tymzr3qitEIR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625119223333,"user_tz":-330,"elapsed":30703,"user":{"displayName":"Harshit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgybYzIih0sZffb4k2rPOU2ZTmImKuEXP5YNWiwYQ=s64","userId":"01054375526835731229"}},"outputId":"72565f3b-9759-468c-a1ba-2052169a5575"},"source":["import pandas as pd\n","import numpy as np\n","import sys\n","import os\n","!pip install ekphrasis\n","!pip install tweet-preprocessor\n","!pip install textblob\n","!python -m textblob.download_corpora\n","\n","dir_of_interest = '/content/drive/MyDrive/Rumor Detection/utils'\n","# modules = {}\n","\n","sys.path.insert(0 , dir_of_interest)\n","# for module in os.listdir(dir_of_interest):\n","#     if '.py' in module and '.pyc' not in module:\n","#         current = module.replace('.py', '')\n","#         modules[current] = __import__(current)\n","\n","\n","from Feature_extraction import *\n","from collect_tweets import *\n","# from clean_data import *\n","\n","import sys\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re, string, unicodedata\n","import nltk\n","from nltk import word_tokenize, sent_tokenize, FreqDist\n","from nltk.corpus import stopwords\n","from nltk.stem import LancasterStemmer, WordNetLemmatizer\n","nltk.download\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","from nltk.tokenize import TweetTokenizer\n","!pip install ekphrasis\n","!pip install tweet-preprocessor\n","import preprocessor as p\n","import re\n","from textblob import TextBlob\n","import seaborn as sns\n","from scipy.stats import pointbiserialr\n","\n","!pip install textblob\n","!python -m textblob.download_corpora\n","\n","%matplotlib inline"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting ekphrasis\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n","\r\u001b[K     |████                            | 10kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 30kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 40kB 10.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 61kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 71kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.4MB/s \n","\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Collecting ujson\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n","\u001b[K     |████████████████████████████████| 184kB 12.7MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n","Collecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n","Building wheels for collected packages: ekphrasis, ftfy\n","  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp37-none-any.whl size=82844 sha256=4a23088110dfa163b9cc303824efd771337d22343193c94bca384580e99f5646\n","  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=46255c5dfbf93c68eb080845259a039a3899e8d0d2db03faf034814bdd2821fd\n","  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n","Successfully built ekphrasis ftfy\n","Installing collected packages: colorama, ujson, ftfy, ekphrasis\n","Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-6.0.3 ujson-4.0.2\n","Collecting tweet-preprocessor\n","  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n","Installing collected packages: tweet-preprocessor\n","Successfully installed tweet-preprocessor-0.6.0\n","Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/conll2000.zip.\n","[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n","Finished.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.1)\n","Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.0.3)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.4)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n","Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Package conll2000 is already up-to-date!\n","[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Package movie_reviews is already up-to-date!\n","Finished.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LosOj7-ivQpt","executionInfo":{"status":"ok","timestamp":1625119432533,"user_tz":-330,"elapsed":451,"user":{"displayName":"Harshit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgybYzIih0sZffb4k2rPOU2ZTmImKuEXP5YNWiwYQ=s64","userId":"01054375526835731229"}},"outputId":"82b43556-176c-4aa8-b428-c81b59a0052e"},"source":["tc = TweetCollector()\n","tc.get_data_from_id('/content/drive/MyDrive/Rumor Detection/Dataset/live test', [1410339585083772930, 1408837952889933827])"],"execution_count":15,"outputs":[{"output_type":"stream","text":["downloading...: 2it [00:00,  5.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["2 number of tweets collected\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQFxWGRvzJ-P","executionInfo":{"status":"ok","timestamp":1625119434338,"user_tz":-330,"elapsed":504,"user":{"displayName":"Harshit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgybYzIih0sZffb4k2rPOU2ZTmImKuEXP5YNWiwYQ=s64","userId":"01054375526835731229"}},"outputId":"2edb7c87-1b30-4d15-bf18-92fe02eaddee"},"source":["fc = FeatureExtractor('/content/drive/MyDrive/Rumor Detection/Dataset/live test')\n","tweet , user = fc.get_data(None)\n","\n","tweet['created_at'] = tweet['created_at'].apply(lambda x: str(x))\n","user['created_at'] = user['created_at'].apply(lambda x: str(x))\n","user[['followers_count', 'statuses_count','friends_count',\n","      'favourites_count', 'listed_count',]] = user[['followers_count', 'statuses_count','friends_count',\n","      'favourites_count', 'listed_count']].astype(np.int64)\n","\n","tweet[['retweet_count','favorite_count','n_symbols', 'n_user_mentions',\n","       'n_hashtags']] = tweet[['retweet_count','favorite_count','n_symbols', 'n_user_mentions',\n","       'n_hashtags']].astype(np.int64)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Reading...: 100%|██████████| 2/2 [00:00<00:00, 45.08it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"v5aQvh6R2Fg8","executionInfo":{"status":"ok","timestamp":1625119448197,"user_tz":-330,"elapsed":430,"user":{"displayName":"Harshit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgybYzIih0sZffb4k2rPOU2ZTmImKuEXP5YNWiwYQ=s64","userId":"01054375526835731229"}},"outputId":"51b92e16-1fc6-4fb4-ec75-7e61aa026640"},"source":["tweet"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweetId</th>\n","      <th>text</th>\n","      <th>source</th>\n","      <th>created_at</th>\n","      <th>is_reply</th>\n","      <th>retweet_count</th>\n","      <th>favorite_count</th>\n","      <th>is_quote_status</th>\n","      <th>entities_symbols</th>\n","      <th>n_symbols</th>\n","      <th>entities_user_mentions</th>\n","      <th>n_user_mentions</th>\n","      <th>entities_hashtags</th>\n","      <th>n_hashtags</th>\n","      <th>entities_url</th>\n","      <th>n_url</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1410339585083772930</td>\n","      <td>Ex-US defence secretary Donald Rumsfeld dead a...</td>\n","      <td>SocialFlow</td>\n","      <td>2021-06-30 20:48:54</td>\n","      <td>None</td>\n","      <td>86</td>\n","      <td>253</td>\n","      <td>False</td>\n","      <td>[]</td>\n","      <td>0</td>\n","      <td>[]</td>\n","      <td>0</td>\n","      <td>[]</td>\n","      <td>0</td>\n","      <td>[https://bbc.in/3whEA4N]</td>\n","      <td>1</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1408837952889933827</td>\n","      <td>Matt Hancock resigns as health secretary after...</td>\n","      <td>SocialFlow</td>\n","      <td>2021-06-26 17:21:57</td>\n","      <td>None</td>\n","      <td>4802</td>\n","      <td>21701</td>\n","      <td>False</td>\n","      <td>[]</td>\n","      <td>0</td>\n","      <td>[]</td>\n","      <td>0</td>\n","      <td>[]</td>\n","      <td>0</td>\n","      <td>[https://bbc.in/3dka2bU]</td>\n","      <td>1</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               tweetId  ... label\n","0  1410339585083772930  ...  None\n","1  1408837952889933827  ...  None\n","\n","[2 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"id":"KlraX7wr3e6y","executionInfo":{"status":"ok","timestamp":1625119454830,"user_tz":-330,"elapsed":434,"user":{"displayName":"Harshit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgybYzIih0sZffb4k2rPOU2ZTmImKuEXP5YNWiwYQ=s64","userId":"01054375526835731229"}},"outputId":"a505dad9-6d45-437d-a9d5-42c5ee6c7b19"},"source":["user"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userId</th>\n","      <th>name</th>\n","      <th>screen_name</th>\n","      <th>tweetId</th>\n","      <th>verfifed</th>\n","      <th>followers_count</th>\n","      <th>statuses_count</th>\n","      <th>friends_count</th>\n","      <th>favourites_count</th>\n","      <th>listed_count</th>\n","      <th>location</th>\n","      <th>created_at</th>\n","      <th>profile_image_url</th>\n","      <th>profile_background_image_url</th>\n","      <th>default_profile_image</th>\n","      <th>default_profile</th>\n","      <th>profile_use_background_image</th>\n","      <th>url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>742143</td>\n","      <td>BBC News (World)</td>\n","      <td>BBCWorld</td>\n","      <td>1410339585083772930</td>\n","      <td>True</td>\n","      <td>32188761</td>\n","      <td>329553</td>\n","      <td>18</td>\n","      <td>13</td>\n","      <td>124575</td>\n","      <td>London, UK</td>\n","      <td>2007-02-01 07:44:29</td>\n","      <td>http://pbs.twimg.com/profile_images/1150717770...</td>\n","      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>https://t.co/7NEgoMwJy3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5402612</td>\n","      <td>BBC Breaking News</td>\n","      <td>BBCBreaking</td>\n","      <td>1408837952889933827</td>\n","      <td>True</td>\n","      <td>47882735</td>\n","      <td>37316</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>141703</td>\n","      <td>London, UK</td>\n","      <td>2007-04-22 14:42:37</td>\n","      <td>http://pbs.twimg.com/profile_images/1150716997...</td>\n","      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>http://t.co/vBzl7LOaso</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    userId  ...                      url\n","0   742143  ...  https://t.co/7NEgoMwJy3\n","1  5402612  ...   http://t.co/vBzl7LOaso\n","\n","[2 rows x 18 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"X_iQ5Ea0i1Yw","executionInfo":{"status":"ok","timestamp":1625119506623,"user_tz":-330,"elapsed":686,"user":{"displayName":"Harshit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgybYzIih0sZffb4k2rPOU2ZTmImKuEXP5YNWiwYQ=s64","userId":"01054375526835731229"}}},"source":[" def meta_data_preprocessing(tweet, user):\n","  import numpy as np\n","  user['created_at'] = user['created_at'].apply(lambda x : datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n","  tweet['created_at'] = tweet['created_at'].apply(lambda x : datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n","  current_datetime = datetime.now()\n","  user['account_age'] = user['created_at'].apply(lambda x : (current_datetime - x).days)\n","  user['screen_name_len'] = user['screen_name'].apply(len)\n","  user['verfifed'] = user['verfifed'].astype(np.int64)\n","  user['default_profile'] = user['default_profile'].astype(np.int64)\n","  user['default_profile_image'] = user['default_profile_image'].astype(np.int64)\n","  user['profile_use_background_image'] = user['profile_use_background_image'].astype(np.int64)\n","  user['profile_image_url'] = user['profile_image_url'].apply(lambda x : x is not np.nan )\n","  user['profile_image_url'] = user['profile_image_url'].astype(np.int64)\n","  user['profile_background_image_url'] = user['profile_background_image_url'].apply(lambda x : x is not np.nan )\n","  user['profile_background_image_url'] = user['profile_background_image_url'].astype(np.int64)\n","  user['has_location'] = user['location'].apply(lambda x : x is not np.nan)\n","  user['has_location'] = user['has_location'].astype(np.int64)\n","  user['has_url'] = user['url'].apply(lambda x : x is not np.nan)\n","  user['has_url'] = user['has_url'].astype(np.int64)\n","  user[['followers_count', 'statuses_count','friends_count',\n","      'favourites_count', 'listed_count', 'account_age']] = user[['followers_count', 'statuses_count','friends_count',\n","      'favourites_count', 'listed_count', 'account_age']].apply(lambda x  : np.log(x+1))\n","\n","  # cleaning text data \n","\n","  for i,v in enumerate(tweet['text']):\n","    tweet.loc[i,'text'] = p.clean(v)\n","\n","  def preprocess_data(data):\n","    #Removes Numbers\n","    data = data.astype(str).str.replace('\\d+', '')\n","    lower_text = data.str.lower()\n","    lemmatizer = nltk.stem.WordNetLemmatizer()\n","    w_tokenizer =  TweetTokenizer()\n","  \n","    def lemmatize_text(text):\n","      return [(lemmatizer.lemmatize(w)) for w \\\n","                          in w_tokenizer.tokenize((text))]\n","\n","    def remove_punctuation(words):\n","      new_words = []\n","      for word in words:\n","        new_word = re.sub(r'[^\\w\\s]', '', (word))\n","        if new_word != '':\n","          new_words.append(new_word)\n","      return new_words\n","\n","    words = lower_text.apply(lemmatize_text)\n","    words = words.apply(remove_punctuation)\n","    return pd.DataFrame(words)\n","\n","  pre_tweets = preprocess_data(tweet['text'])\n","  tweet['text'] = pre_tweets\n","  stop_words = set(stopwords.words('english'))\n","  tweet['text'] = tweet['text'].apply(lambda x: [item for item in \\\n","                                      x if item not in stop_words])\n","  \n","  print()\n","  def analyze_sentiment(tweet):\n","    analysis = TextBlob(tweet)\n","    return analysis.sentiment.polarity\n","\n","  tweet['polarity'] =  tweet['text'].apply(lambda x : analyze_sentiment(' '.join(x)))\n","  tweet['is_reply'] = tweet['is_reply'].isnull()\n","  tweet['is_reply'] = tweet['is_reply'].apply(lambda x : not x)\n","  tweet['is_reply'] = tweet['is_reply'].astype(np.int64)\n","  tweet['is_quote_status'] = tweet['is_quote_status'].astype(np.int64)\n","  tweet['text_len'] = tweet['text'].apply(len)\n","  tweet['post_age'] = tweet['created_at'].apply(lambda x : (current_datetime-x).days)\n","  tweet[['retweet_count','favorite_count','n_symbols', 'n_user_mentions',\n","       'n_hashtags']] = tweet[['retweet_count','favorite_count','n_symbols', 'n_user_mentions',\n","       'n_hashtags']].apply(lambda x : np.log(x+1))\n","  merge = tweet.merge(user, on='tweetId', suffixes=['_tweet', '_user'])\n","  merge['posted_in'] = merge['created_at_tweet'] - merge['created_at_user']\n","  merge['posted_in'] =  merge['posted_in'].apply(lambda x : x.days)\n","  df = merge[['is_reply','verfifed', 'followers_count',\n","       'retweet_count', 'favorite_count', 'is_quote_status', 'n_symbols', \n","       'n_user_mentions', 'n_hashtags','n_url', 'polarity', 'text_len', 'post_age',\n","       'statuses_count', 'friends_count', 'favourites_count', 'listed_count',\n","       'profile_image_url', 'profile_background_image_url', 'default_profile_image',\n","       'default_profile', 'profile_use_background_image', 'account_age',\n","       'screen_name_len', 'has_location', 'has_url', 'posted_in', 'label']]\n","  # df.loc[:, 'label'] = df['label'].replace({'non-rumor':False, 'true':True, 'false':True, 'unverified':True})\n","  selected_features = ['profile_use_background_image','profile_background_image_url',\n","                     'default_profile_image','default_profile','verfifed', 'text_len',\n","                     'n_url', 'post_age', 'statuses_count', 'listed_count', 'n_symbols',\n","                     'posted_in', 'n_user_mentions', 'polarity', 'favourites_count', 'favorite_count',\n","                     'screen_name_len', 'n_hashtags']\n","  sel_num_features = ['text_len','n_url', 'post_age', 'statuses_count', 'listed_count', 'n_symbols',\n","                     'posted_in', 'n_user_mentions', 'polarity', 'favourites_count', 'favorite_count',\n","                     'screen_name_len', 'n_hashtags']\n","\n","  sel_cat_features = ['profile_use_background_image','profile_background_image_url',\n","                      'default_profile_image','default_profile','verfifed']\n","  X = df[selected_features]\n","  y = df['label']\n","  print(X)\n","  from pickle import load\n","  sc_x = load(open('/content/drive/MyDrive/Rumor Detection/Scaler/scaler.pkl', 'rb'))\n","  X[sel_num_features] = sc_x.transform(X[sel_num_features])\n","  return X, y"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZxSzMcA3g0h","colab":{"base_uri":"https://localhost:8080/","height":434},"executionInfo":{"status":"error","timestamp":1625119506624,"user_tz":-330,"elapsed":10,"user":{"displayName":"Harshit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgybYzIih0sZffb4k2rPOU2ZTmImKuEXP5YNWiwYQ=s64","userId":"01054375526835731229"}},"outputId":"d914ba7f-b866-4bb0-f82b-5bd27874537f"},"source":["X_user_tweet, y_user_tweet = meta_data_preprocessing(tweet.copy(), user.copy())\n","X_user_tweet"],"execution_count":26,"outputs":[{"output_type":"stream","text":["\n","Empty DataFrame\n","Columns: [profile_use_background_image, profile_background_image_url, default_profile_image, default_profile, verfifed, text_len, n_url, post_age, statuses_count, listed_count, n_symbols, posted_in, n_user_mentions, polarity, favourites_count, favorite_count, screen_name_len, n_hashtags]\n","Index: []\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-3f663ab6ea96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_user_tweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_user_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_data_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_user_tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-e19dd07212f9>\u001b[0m in \u001b[0;36mmeta_data_preprocessing\u001b[0;34m(tweet, user)\u001b[0m\n\u001b[1;32m     96\u001b[0m  \u001b[0;32mfrom\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m  \u001b[0msc_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Rumor Detection/Scaler/scaler.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m  \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msel_num_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msel_num_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m  \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    793\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy,\n\u001b[1;32m    794\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    584\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[0;32m--> 586\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required by StandardScaler."]}]},{"cell_type":"code","metadata":{"id":"kieyglDEiCkv"},"source":[""],"execution_count":null,"outputs":[]}]}